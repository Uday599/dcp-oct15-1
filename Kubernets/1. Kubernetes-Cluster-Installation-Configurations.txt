Kube-Master(Controller)
	Kube-Worker1
	Kube-Worker2
	
	
WE ARE SETTING UP KUBERNETES USING - KUBEADM
-------------------------------------------
	
Add Port: 0 - 65535  -- Open this posts in inbound rule of security group 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To change the Hostname simply do,
--------------------------------


vi /etc/hostname
# give a name
run init 6 # to reboot

On Both Master and Worker Nodes:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

sudo -i

yum update -y

swapoff -a
#The Kubernetes scheduler determines the best available node on which to deploy newly created pods. If memory swapping is allowed to occur on a host system, this can lead to performance and stability issues within Kubernetes.

setenforce 0
#Disabling the SElinux makes all containers can easily access host filesystem.

yum install docker -y

systemctl enable docker 
systemctl start docker

cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=0
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
exclude=kube*
EOF


cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

sysctl --system

> sysctl is used to modify kernel parameters at runtime. The parameters available are those listed under /proc/sys/. Procfs is required for sysctl support in Linux. You can use sysctl to both read and write sysctl data.

yum install -y kubeadm-1.21.3 kubelet-1.21.3 kubectl-1.21.3 --disableexcludes=kubernetes 

systemctl enable kubelet 
systemctl start kubelet

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Only on Master Node:
~~~~~~~~~~~~~~~~~~~~~

sudo kubeadm init --apiserver-advertise-address=172.31.12.246 --pod-network-cidr=192.168.0.0/16 --ignore-preflight-errors=NumCPU --ignore-preflight-errors=Mem

Above command : here we are initializing out one of the VM as Master - Private IP , POD CIDR is defined , ignore is to neglect any runtime error like CPU error and Memory and treat them as warning

[WARNING NumCPU]: the number of available CPUs 1 is less than the required 2
[WARNING Mem]: the system RAM (964 MB) is less than the minimum 1700 MB


--------------------------------# this can be seen in welcome page only

mkdir -p $HOME/.kube   -> this is in root directory
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config   -> we are just copying the repo file which we installed earlier(upper part) to required path
sudo chown $(id -u):$(id -g) $HOME/.kube/config  -> changing the ownership


#export KUBECONFIG=/etc/kubernetes/kubelet.conf

#We need to install a flannel network plugin to run coredns to start pod network communication.
# we have Kubeproxy for networking in PODS  - Network Pluggin
#FLANNER - pluggin for network adapter

sudo kubectl apply -f https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml 
sudo kubectl apply -f https://docs.projectcalico.org/v3.8/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml 

# Kubelet -> tool to manage cluster like creating containers/destroying etc
# Kubectl -> CLI
# kubeadm -> to create/configure cluster

Test the configuration ::

kubectl get pods --all-namespaces


[root@ip-172-31-11-76 ~]# kubectl get pods --all-namespaces
NAMESPACE     NAME                                                                  READY   STATUS              RESTARTS   AGE
kube-system   calico-kube-controllers-7f6768fdfb-9xvz8                              0/1     ContainerCreating   0          46s
kube-system   calico-node-pvh89                                                     0/1     PodInitializing     0          46s
kube-system   coredns-558bd4d5db-97hjb                                              0/1     ContainerCreating   0          5m48s
kube-system   coredns-558bd4d5db-whkv9                                              0/1     Pending             0          5m48s
kube-system   etcd-ip-172-31-11-76.ap-south-1.compute.internal                      1/1     Running             0          6m1s
kube-system   kube-apiserver-ip-172-31-11-76.ap-south-1.compute.internal            1/1     Running             0          6m1s
kube-system   kube-controller-manager-ip-172-31-11-76.ap-south-1.compute.internal   1/1     Running             0          6m1s
kube-system   kube-proxy-jvsd2                                                      1/1     Running             0          5m49s
kube-system   kube-scheduler-ip-172-31-11-76.ap-south-1.compute.internal            1/1     Running             0          6m1s


kubectl get nodes

[root@ip-172-31-11-76 ~]# kubectl get nodes
NAME                                          STATUS   ROLES                  AGE     VERSION
ip-172-31-11-76.ap-south-1.compute.internal   Ready    control-plane,master   6m23s   v1.21.3


kubectl describe nodes  # to see the deatiled view about nodes


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Execute the below commmand in Worker Nodes, to join all the worker nodes with Master :
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

> execute below command in every worker node to connect to master
> you can verify by kubectl get nodes -> to view the nodes
> Below command should be copied while initializing kubeadm in master (check upper part :)

> execute below command to get join token in kubemaster

kubeadm token create --print-join-command

Sample:

kubeadm join 172.31.13.1:6443 --token db44h7.mld38h3otcgpgs6s --discovery-token-ca-cert-hash sha256:35e4c6d99284116035840e1900712cf00c1850ceaac8318c78164f3ea9183c86

##########################
Make sure to allow all traffic
##########################


Where will Namespace come into picture - cluster?

--------------Sunday 13 Nov intro --------------

KUBECTL SYNTAX
--------------

kubectl <command> <type> <name> <flags>

Typs -> objects like pods/deployments/replicaset many more
command -> get create descrive many more
flag is optional
name is name of the object


> kubectl get pods --all-namespace

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# HOW DO WE SET A STATIC HOSTNAME TO A SERVER IN AWS
-----------------------------------------------------

1. Other way- simple:
```
  vi /etc/hostname # chnage the hostname
  init 6	   # reboot
```



[root@ip-172-31-11-68 ~]# hostnamectl
   Static hostname: ip-172-31-11-68.ap-south-1.compute.internal
         Icon name: computer-vm
           Chassis: vm
        Machine ID: 365783bc4b04418fa2831c895c890a6a
           Boot ID: 953e0c98f4f148aabb96a0ba1f911989
    Virtualization: xen
  Operating System: Amazon Linux 2
       CPE OS Name: cpe:2.3:o:amazon:amazon_linux:2
            Kernel: Linux 5.10.157-139.675.amzn2.x86_64
      Architecture: x86-64
      
[root@ip-172-31-11-68 ~]# sudo hostnamectl set-hostname --static Worker1

[root@ip-172-31-11-68 ~]# sudo vi /etc/cloud/cloud.cfg

# /etc/cloud/cloud.cfg.d

users:
 - default

disable_root: true
ssh_pwauth:   false

mount_default_fields: [~, ~, 'auto', 'defaults,nofail', '0', '2']
resize_rootfs: noblock
resize_rootfs_tmp: /dev
ssh_deletekeys:   true
ssh_genkeytypes:  ~
syslog_fix_perms: ~

datasource_list: [ Ec2, None ]
repo_upgrade: security
repo_upgrade_exclude:
 - kernel
 - nvidia*
 - cuda*

# Might interfere with ec2-net-utils
network:
  config: disabled

cloud_init_modules:
 - migrator
 - bootcmd
 - write-files
 - write-metadata
 - amazonlinux_repo_https
 - growpart
 - resizefs
 - set-hostname
 - update-hostname
 - update-etc-hosts
 - rsyslog
 - users-groups
 - ssh
 - resolv-conf

cloud_config_modules:
 - disk_setup
 - mounts
 - locale
 - set-passwords
 - yum-configure
 - yum-add-repo
 - package-update-upgrade-install
 - timezone
 - disable-ec2-metadata
 - runcmd

cloud_final_modules:
 - scripts-per-once
 - scripts-per-boot
 - scripts-per-instance
 - scripts-user
 - ssh-authkey-fingerprints
 - keys-to-console
 - phone-home
 - final-message
 - power-state-change

system_info:
  # This will affect which distro class gets used
  distro: amazon
  distro_short: amzn
  default_user:
    name: ec2-user
    lock_passwd: true
    gecos: EC2 Default User
    groups: [wheel, adm, systemd-journal]
    sudo: ["ALL=(ALL) NOPASSWD:ALL"]
    shell: /bin/bash
  paths:
    cloud_dir: /var/lib/cloud
    templates_dir: /etc/cloud/templates
  ssh_svcname: sshd

mounts:
 - [ ephemeral0, /media/ephemeral0 ]
 - [ swap, none, swap, sw, "0", "0" ]
# vim:syntax=yaml

preserve_hostname: true					# set this parameter



[root@ip-172-31-11-68 ~]# hostname
Worker1

[root@ip-172-31-11-68 ~]# exit
logout

[ec2-user@ip-172-31-11-68 ~]$ sudo -i
[root@Worker1 ~]#

this will keep your server hostname to Worker1 even after you restart your instance
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

# to delete pod forcefully
=========================-
 kubectl delete pod --force <pod_name>

# To remove worker node from cluster , do this in master first:

  1.  kubectl get nodes
  2.  kubectl drain < node-name > --ignore-daemonsets
  3.  kubectl delete node < node-name >
  

# kubeadm reset -> Run this in worker node if your facing any issue in connecting to cluster

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++=





